{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 😷 Masked Face Recognition Training Notebook\n",
    "This notebook trains both CNN and Transformer (EfficientNetB0) models for 4 tasks: emotion, age, gender, ethnicity.\n",
    "It is configured to run **100 epochs** for each task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn(num_classes, input_shape=(64,64,3)):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv2D(32, (3,3), activation=\"relu\", padding=\"same\", input_shape=input_shape),\n",
    "        tf.keras.layers.MaxPooling2D(2,2),\n",
    "        tf.keras.layers.Conv2D(64, (3,3), activation=\"relu\", padding=\"same\"),\n",
    "        tf.keras.layers.MaxPooling2D(2,2),\n",
    "        tf.keras.layers.Conv2D(128, (3,3), activation=\"relu\", padding=\"same\"),\n",
    "        tf.keras.layers.MaxPooling2D(2,2),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(256, activation=\"relu\"),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(num_classes, activation=\"softmax\")\n",
    "    ])\n",
    "    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "def build_transformer(num_classes, input_shape=(128,128,3)):\n",
    "    try:\n",
    "        base = tf.keras.applications.EfficientNetB0(\n",
    "            input_shape=input_shape,\n",
    "            include_top=False,\n",
    "            weights=\"imagenet\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(\" Falling back to random initialization:\", e)\n",
    "        base = tf.keras.applications.EfficientNetB0(\n",
    "            input_shape=input_shape,\n",
    "            include_top=False,\n",
    "            weights=None\n",
    "        )\n",
    "\n",
    "    # Freeze most layers\n",
    "    for layer in base.layers[:-20]:\n",
    "        layer.trainable = False\n",
    "\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    x = base(inputs, training=False)\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "    x = tf.keras.layers.Dense(256, activation=\"relu\")(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    outputs = tf.keras.layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs, outputs, name=\"EfficientNetB0_Finetuned\")\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def prepare_data(df, label_col, img_size=(64,64)):\n",
    "    X = np.array([np.fromstring(p, sep=\" \").reshape(48,48) for p in df['pixels']])\n",
    "    X = np.expand_dims(X, -1)\n",
    "    X = np.repeat(X, 3, axis=-1)\n",
    "    X = np.array([tf.image.resize(img, img_size).numpy() for img in X])\n",
    "    X = X.astype(\"float32\") / 255.0\n",
    "    labels = df[label_col].values\n",
    "    le = LabelEncoder()\n",
    "    y = to_categorical(le.fit_transform(labels))\n",
    "    classes = le.classes_\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "        X, y, test_size=0.2, stratify=y, random_state=42\n",
    "    )\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42\n",
    "    )\n",
    "    return (X_train, y_train), (X_val, y_val), (X_test, y_test), classes\n",
    "\n",
    "def train_and_save(model, X_train, y_train, X_val, y_val,\n",
    "                   model_name, classes, epochs=15, batch_size=64):\n",
    "    callbacks = [\n",
    "        ReduceLROnPlateau(monitor=\"val_accuracy\", factor=0.7, patience=3, verbose=1),\n",
    "        EarlyStopping(monitor=\"val_accuracy\", patience=10, restore_best_weights=True, verbose=1),\n",
    "        ModelCheckpoint(f\"models/{model_name}.keras\", save_best_only=True, monitor=\"val_accuracy\", verbose=1)\n",
    "    ]\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        verbose=1,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "    os.makedirs(\"models\", exist_ok=True)\n",
    "    with open(f\"models/{model_name}_classes.pkl\", \"wb\") as f:\n",
    "        pickle.dump(list(classes), f)\n",
    "    with open(f\"models/{model_name}_history.json\", \"w\") as f:\n",
    "        json.dump(history.history, f)\n",
    "    print(f\"Saved {model_name}\")\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training emotion (CNN)\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\masked_face_emotion_project_light\\.venv\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.1752 - loss: 1.9305\n",
      "Epoch 1: val_accuracy improved from None to 0.31857, saving model to models/emotion.keras\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 78ms/step - accuracy: 0.2183 - loss: 1.8781 - val_accuracy: 0.3186 - val_loss: 1.7258 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m262/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.3318 - loss: 1.7023\n",
      "Epoch 2: val_accuracy improved from 0.31857 to 0.37238, saving model to models/emotion.keras\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 80ms/step - accuracy: 0.3541 - loss: 1.6553 - val_accuracy: 0.3724 - val_loss: 1.5826 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m262/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 0.4368 - loss: 1.4843\n",
      "Epoch 3: val_accuracy improved from 0.37238 to 0.48810, saving model to models/emotion.keras\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 81ms/step - accuracy: 0.4530 - loss: 1.4511 - val_accuracy: 0.4881 - val_loss: 1.3540 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m262/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 0.5264 - loss: 1.2759\n",
      "Epoch 4: val_accuracy improved from 0.48810 to 0.56571, saving model to models/emotion.keras\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 81ms/step - accuracy: 0.5363 - loss: 1.2453 - val_accuracy: 0.5657 - val_loss: 1.1894 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m262/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.6082 - loss: 1.0562\n",
      "Epoch 5: val_accuracy improved from 0.56571 to 0.64810, saving model to models/emotion.keras\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 85ms/step - accuracy: 0.6157 - loss: 1.0454 - val_accuracy: 0.6481 - val_loss: 1.0264 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m262/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.6923 - loss: 0.8542\n",
      "Epoch 6: val_accuracy improved from 0.64810 to 0.70762, saving model to models/emotion.keras\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 83ms/step - accuracy: 0.6958 - loss: 0.8477 - val_accuracy: 0.7076 - val_loss: 0.8637 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m262/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.7497 - loss: 0.7120\n",
      "Epoch 7: val_accuracy improved from 0.70762 to 0.73238, saving model to models/emotion.keras\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 83ms/step - accuracy: 0.7511 - loss: 0.7106 - val_accuracy: 0.7324 - val_loss: 0.8197 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m262/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.7923 - loss: 0.5800\n",
      "Epoch 8: val_accuracy improved from 0.73238 to 0.78429, saving model to models/emotion.keras\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 83ms/step - accuracy: 0.7921 - loss: 0.5864 - val_accuracy: 0.7843 - val_loss: 0.7297 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m262/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.8280 - loss: 0.4784\n",
      "Epoch 9: val_accuracy improved from 0.78429 to 0.78619, saving model to models/emotion.keras\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 83ms/step - accuracy: 0.8266 - loss: 0.4853 - val_accuracy: 0.7862 - val_loss: 0.7265 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m262/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.8558 - loss: 0.3983\n",
      "Epoch 10: val_accuracy improved from 0.78619 to 0.80048, saving model to models/emotion.keras\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 83ms/step - accuracy: 0.8545 - loss: 0.4058 - val_accuracy: 0.8005 - val_loss: 0.7461 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m262/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.8702 - loss: 0.3589\n",
      "Epoch 11: val_accuracy improved from 0.80048 to 0.81619, saving model to models/emotion.keras\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 83ms/step - accuracy: 0.8677 - loss: 0.3653 - val_accuracy: 0.8162 - val_loss: 0.7169 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m262/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.8889 - loss: 0.3055\n",
      "Epoch 12: val_accuracy improved from 0.81619 to 0.82286, saving model to models/emotion.keras\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 84ms/step - accuracy: 0.8858 - loss: 0.3185 - val_accuracy: 0.8229 - val_loss: 0.6827 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.9024 - loss: 0.2728\n",
      "Epoch 13: val_accuracy improved from 0.82286 to 0.84000, saving model to models/emotion.keras\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 86ms/step - accuracy: 0.9026 - loss: 0.2709 - val_accuracy: 0.8400 - val_loss: 0.6898 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m262/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.9055 - loss: 0.2597\n",
      "Epoch 14: val_accuracy improved from 0.84000 to 0.84190, saving model to models/emotion.keras\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 84ms/step - accuracy: 0.9039 - loss: 0.2656 - val_accuracy: 0.8419 - val_loss: 0.6941 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "\u001b[1m262/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.9212 - loss: 0.2213\n",
      "Epoch 15: val_accuracy improved from 0.84190 to 0.84476, saving model to models/emotion.keras\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 83ms/step - accuracy: 0.9145 - loss: 0.2416 - val_accuracy: 0.8448 - val_loss: 0.7387 - learning_rate: 0.0010\n",
      "Epoch 16/50\n",
      "\u001b[1m262/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.9273 - loss: 0.2081\n",
      "Epoch 16: val_accuracy improved from 0.84476 to 0.84667, saving model to models/emotion.keras\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 84ms/step - accuracy: 0.9223 - loss: 0.2183 - val_accuracy: 0.8467 - val_loss: 0.7813 - learning_rate: 0.0010\n",
      "Epoch 17/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.9316 - loss: 0.1945\n",
      "Epoch 17: val_accuracy did not improve from 0.84667\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 85ms/step - accuracy: 0.9278 - loss: 0.2040 - val_accuracy: 0.8457 - val_loss: 0.7881 - learning_rate: 0.0010\n",
      "Epoch 18/50\n",
      "\u001b[1m262/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.9280 - loss: 0.1960\n",
      "Epoch 18: val_accuracy did not improve from 0.84667\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 83ms/step - accuracy: 0.9295 - loss: 0.1923 - val_accuracy: 0.8452 - val_loss: 0.8095 - learning_rate: 0.0010\n",
      "Epoch 19/50\n",
      "\u001b[1m262/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.9362 - loss: 0.1767\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 0.0007000000332482159.\n",
      "\n",
      "Epoch 19: val_accuracy did not improve from 0.84667\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 87ms/step - accuracy: 0.9367 - loss: 0.1736 - val_accuracy: 0.8371 - val_loss: 0.9025 - learning_rate: 0.0010\n",
      "Epoch 20/50\n",
      "\u001b[1m262/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.9504 - loss: 0.1366\n",
      "Epoch 20: val_accuracy improved from 0.84667 to 0.84762, saving model to models/emotion.keras\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 86ms/step - accuracy: 0.9527 - loss: 0.1312 - val_accuracy: 0.8476 - val_loss: 0.8867 - learning_rate: 7.0000e-04\n",
      "Epoch 21/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.9587 - loss: 0.1143\n",
      "Epoch 21: val_accuracy improved from 0.84762 to 0.85190, saving model to models/emotion.keras\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 88ms/step - accuracy: 0.9579 - loss: 0.1178 - val_accuracy: 0.8519 - val_loss: 0.9371 - learning_rate: 7.0000e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m262/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.9563 - loss: 0.1181\n",
      "Epoch 22: val_accuracy did not improve from 0.85190\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 87ms/step - accuracy: 0.9567 - loss: 0.1203 - val_accuracy: 0.8514 - val_loss: 0.8909 - learning_rate: 7.0000e-04\n",
      "Epoch 23/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.9627 - loss: 0.1091\n",
      "Epoch 23: val_accuracy did not improve from 0.85190\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 89ms/step - accuracy: 0.9614 - loss: 0.1088 - val_accuracy: 0.8457 - val_loss: 0.9933 - learning_rate: 7.0000e-04\n",
      "Epoch 24/50\n",
      "\u001b[1m262/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.9588 - loss: 0.1131\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 0.0004900000232737511.\n",
      "\n",
      "Epoch 24: val_accuracy did not improve from 0.85190\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 90ms/step - accuracy: 0.9582 - loss: 0.1156 - val_accuracy: 0.8481 - val_loss: 0.9362 - learning_rate: 7.0000e-04\n",
      "Epoch 25/50\n",
      "\u001b[1m262/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.9688 - loss: 0.0852\n",
      "Epoch 25: val_accuracy did not improve from 0.85190\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 90ms/step - accuracy: 0.9693 - loss: 0.0853 - val_accuracy: 0.8495 - val_loss: 0.9761 - learning_rate: 4.9000e-04\n",
      "Epoch 26/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.9670 - loss: 0.0903\n",
      "Epoch 26: val_accuracy did not improve from 0.85190\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 91ms/step - accuracy: 0.9708 - loss: 0.0808 - val_accuracy: 0.8462 - val_loss: 1.0255 - learning_rate: 4.9000e-04\n",
      "Epoch 27/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.9717 - loss: 0.0786\n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 0.00034300000406801696.\n",
      "\n",
      "Epoch 27: val_accuracy did not improve from 0.85190\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 91ms/step - accuracy: 0.9714 - loss: 0.0808 - val_accuracy: 0.8481 - val_loss: 1.0512 - learning_rate: 4.9000e-04\n",
      "Epoch 28/50\n",
      "\u001b[1m262/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.9738 - loss: 0.0705\n",
      "Epoch 28: val_accuracy improved from 0.85190 to 0.85476, saving model to models/emotion.keras\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 95ms/step - accuracy: 0.9761 - loss: 0.0644 - val_accuracy: 0.8548 - val_loss: 1.0793 - learning_rate: 3.4300e-04\n",
      "Epoch 29/50\n",
      "\u001b[1m262/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.9770 - loss: 0.0600\n",
      "Epoch 29: val_accuracy did not improve from 0.85476\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 95ms/step - accuracy: 0.9768 - loss: 0.0615 - val_accuracy: 0.8495 - val_loss: 1.1185 - learning_rate: 3.4300e-04\n",
      "Epoch 30/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.9787 - loss: 0.0541\n",
      "Epoch 30: val_accuracy did not improve from 0.85476\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 95ms/step - accuracy: 0.9775 - loss: 0.0580 - val_accuracy: 0.8486 - val_loss: 1.0949 - learning_rate: 3.4300e-04\n",
      "Epoch 31/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.9802 - loss: 0.0509\n",
      "Epoch 31: ReduceLROnPlateau reducing learning rate to 0.00024009999469853935.\n",
      "\n",
      "Epoch 31: val_accuracy did not improve from 0.85476\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 93ms/step - accuracy: 0.9795 - loss: 0.0541 - val_accuracy: 0.8476 - val_loss: 1.0828 - learning_rate: 3.4300e-04\n",
      "Epoch 32/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.9806 - loss: 0.0540\n",
      "Epoch 32: val_accuracy did not improve from 0.85476\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 94ms/step - accuracy: 0.9817 - loss: 0.0507 - val_accuracy: 0.8514 - val_loss: 1.1558 - learning_rate: 2.4010e-04\n",
      "Epoch 33/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - accuracy: 0.9831 - loss: 0.0464\n",
      "Epoch 33: val_accuracy did not improve from 0.85476\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 103ms/step - accuracy: 0.9822 - loss: 0.0485 - val_accuracy: 0.8529 - val_loss: 1.1771 - learning_rate: 2.4010e-04\n",
      "Epoch 34/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.9826 - loss: 0.0460\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 0.00016806999628897755.\n",
      "\n",
      "Epoch 34: val_accuracy did not improve from 0.85476\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 96ms/step - accuracy: 0.9830 - loss: 0.0447 - val_accuracy: 0.8500 - val_loss: 1.1742 - learning_rate: 2.4010e-04\n",
      "Epoch 35/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.9816 - loss: 0.0507\n",
      "Epoch 35: val_accuracy did not improve from 0.85476\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 92ms/step - accuracy: 0.9831 - loss: 0.0465 - val_accuracy: 0.8524 - val_loss: 1.2406 - learning_rate: 1.6807e-04\n",
      "Epoch 36/50\n",
      "\u001b[1m262/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.9849 - loss: 0.0390\n",
      "Epoch 36: val_accuracy improved from 0.85476 to 0.85524, saving model to models/emotion.keras\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 93ms/step - accuracy: 0.9842 - loss: 0.0421 - val_accuracy: 0.8552 - val_loss: 1.2066 - learning_rate: 1.6807e-04\n",
      "Epoch 37/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.9862 - loss: 0.0384\n",
      "Epoch 37: val_accuracy did not improve from 0.85524\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 92ms/step - accuracy: 0.9842 - loss: 0.0412 - val_accuracy: 0.8514 - val_loss: 1.2484 - learning_rate: 1.6807e-04\n",
      "Epoch 38/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.9859 - loss: 0.0379\n",
      "Epoch 38: val_accuracy did not improve from 0.85524\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 96ms/step - accuracy: 0.9855 - loss: 0.0381 - val_accuracy: 0.8543 - val_loss: 1.2423 - learning_rate: 1.6807e-04\n",
      "Epoch 39/50\n",
      "\u001b[1m262/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.9875 - loss: 0.0347\n",
      "Epoch 39: ReduceLROnPlateau reducing learning rate to 0.00011764899536501615.\n",
      "\n",
      "Epoch 39: val_accuracy did not improve from 0.85524\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 96ms/step - accuracy: 0.9875 - loss: 0.0347 - val_accuracy: 0.8529 - val_loss: 1.2779 - learning_rate: 1.6807e-04\n",
      "Epoch 40/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.9872 - loss: 0.0348\n",
      "Epoch 40: val_accuracy did not improve from 0.85524\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m880s\u001b[0m 3s/step - accuracy: 0.9866 - loss: 0.0359 - val_accuracy: 0.8495 - val_loss: 1.3079 - learning_rate: 1.1765e-04\n",
      "Epoch 41/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 195ms/step - accuracy: 0.9862 - loss: 0.0384\n",
      "Epoch 41: val_accuracy did not improve from 0.85524\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 200ms/step - accuracy: 0.9862 - loss: 0.0347 - val_accuracy: 0.8500 - val_loss: 1.3581 - learning_rate: 1.1765e-04\n",
      "Epoch 42/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.9874 - loss: 0.0347\n",
      "Epoch 42: ReduceLROnPlateau reducing learning rate to 8.235429777414538e-05.\n",
      "\n",
      "Epoch 42: val_accuracy did not improve from 0.85524\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 116ms/step - accuracy: 0.9870 - loss: 0.0341 - val_accuracy: 0.8505 - val_loss: 1.3265 - learning_rate: 1.1765e-04\n",
      "Epoch 43/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.9883 - loss: 0.0301\n",
      "Epoch 43: val_accuracy did not improve from 0.85524\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 112ms/step - accuracy: 0.9887 - loss: 0.0309 - val_accuracy: 0.8543 - val_loss: 1.3556 - learning_rate: 8.2354e-05\n",
      "Epoch 44/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - accuracy: 0.9898 - loss: 0.0291\n",
      "Epoch 44: val_accuracy did not improve from 0.85524\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 112ms/step - accuracy: 0.9898 - loss: 0.0284 - val_accuracy: 0.8505 - val_loss: 1.3421 - learning_rate: 8.2354e-05\n",
      "Epoch 45/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.9864 - loss: 0.0329\n",
      "Epoch 45: ReduceLROnPlateau reducing learning rate to 5.76480058953166e-05.\n",
      "\n",
      "Epoch 45: val_accuracy did not improve from 0.85524\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 112ms/step - accuracy: 0.9873 - loss: 0.0316 - val_accuracy: 0.8500 - val_loss: 1.3543 - learning_rate: 8.2354e-05\n",
      "Epoch 46/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - accuracy: 0.9900 - loss: 0.0284\n",
      "Epoch 46: val_accuracy did not improve from 0.85524\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 112ms/step - accuracy: 0.9887 - loss: 0.0291 - val_accuracy: 0.8519 - val_loss: 1.3514 - learning_rate: 5.7648e-05\n",
      "Epoch 46: early stopping\n",
      "Restoring model weights from the end of the best epoch: 36.\n",
      "Saved emotion\n",
      "\n",
      " Training age (CNN)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\masked_face_emotion_project_light\\.venv\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.6058 - loss: 0.8046\n",
      "Epoch 1: val_accuracy improved from None to 0.69571, saving model to models/age.keras\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 111ms/step - accuracy: 0.6467 - loss: 0.7304 - val_accuracy: 0.6957 - val_loss: 0.6526 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.7089 - loss: 0.6289\n",
      "Epoch 2: val_accuracy improved from 0.69571 to 0.74095, saving model to models/age.keras\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 110ms/step - accuracy: 0.7099 - loss: 0.6222 - val_accuracy: 0.7410 - val_loss: 0.5832 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.7430 - loss: 0.5623\n",
      "Epoch 3: val_accuracy improved from 0.74095 to 0.77000, saving model to models/age.keras\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 111ms/step - accuracy: 0.7496 - loss: 0.5495 - val_accuracy: 0.7700 - val_loss: 0.5128 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.7797 - loss: 0.4870\n",
      "Epoch 4: val_accuracy improved from 0.77000 to 0.80143, saving model to models/age.keras\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 112ms/step - accuracy: 0.7861 - loss: 0.4794 - val_accuracy: 0.8014 - val_loss: 0.4555 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - accuracy: 0.8267 - loss: 0.3871\n",
      "Epoch 5: val_accuracy improved from 0.80143 to 0.83476, saving model to models/age.keras\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 114ms/step - accuracy: 0.8289 - loss: 0.3886 - val_accuracy: 0.8348 - val_loss: 0.3983 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.8680 - loss: 0.3172\n",
      "Epoch 6: val_accuracy improved from 0.83476 to 0.86619, saving model to models/age.keras\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 111ms/step - accuracy: 0.8699 - loss: 0.3134 - val_accuracy: 0.8662 - val_loss: 0.3601 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.9023 - loss: 0.2361\n",
      "Epoch 7: val_accuracy improved from 0.86619 to 0.88190, saving model to models/age.keras\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 112ms/step - accuracy: 0.9026 - loss: 0.2404 - val_accuracy: 0.8819 - val_loss: 0.3261 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.9269 - loss: 0.1932\n",
      "Epoch 8: val_accuracy improved from 0.88190 to 0.90095, saving model to models/age.keras\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 112ms/step - accuracy: 0.9283 - loss: 0.1882 - val_accuracy: 0.9010 - val_loss: 0.3303 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9434 - loss: 0.1472\n",
      "Epoch 9: val_accuracy improved from 0.90095 to 0.90857, saving model to models/age.keras\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 111ms/step - accuracy: 0.9445 - loss: 0.1463 - val_accuracy: 0.9086 - val_loss: 0.3219 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - accuracy: 0.9592 - loss: 0.1112\n",
      "Epoch 10: val_accuracy improved from 0.90857 to 0.92286, saving model to models/age.keras\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 118ms/step - accuracy: 0.9584 - loss: 0.1139 - val_accuracy: 0.9229 - val_loss: 0.3125 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.9627 - loss: 0.0975\n",
      "Epoch 11: val_accuracy did not improve from 0.92286\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 111ms/step - accuracy: 0.9617 - loss: 0.0999 - val_accuracy: 0.9148 - val_loss: 0.3413 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - accuracy: 0.9711 - loss: 0.0764\n",
      "Epoch 12: val_accuracy did not improve from 0.92286\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 114ms/step - accuracy: 0.9716 - loss: 0.0759 - val_accuracy: 0.9200 - val_loss: 0.3474 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - accuracy: 0.9760 - loss: 0.0671\n",
      "Epoch 13: val_accuracy improved from 0.92286 to 0.92857, saving model to models/age.keras\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 145ms/step - accuracy: 0.9765 - loss: 0.0659 - val_accuracy: 0.9286 - val_loss: 0.3947 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step - accuracy: 0.9815 - loss: 0.0508\n",
      "Epoch 14: val_accuracy did not improve from 0.92857\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 190ms/step - accuracy: 0.9807 - loss: 0.0536 - val_accuracy: 0.9281 - val_loss: 0.3912 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step - accuracy: 0.9826 - loss: 0.0478\n",
      "Epoch 15: val_accuracy improved from 0.92857 to 0.92952, saving model to models/age.keras\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 190ms/step - accuracy: 0.9805 - loss: 0.0517 - val_accuracy: 0.9295 - val_loss: 0.4423 - learning_rate: 0.0010\n",
      "Epoch 16/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step - accuracy: 0.9806 - loss: 0.0536\n",
      "Epoch 16: val_accuracy improved from 0.92952 to 0.93238, saving model to models/age.keras\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 190ms/step - accuracy: 0.9816 - loss: 0.0506 - val_accuracy: 0.9324 - val_loss: 0.4104 - learning_rate: 0.0010\n",
      "Epoch 17/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step - accuracy: 0.9837 - loss: 0.0496\n",
      "Epoch 17: val_accuracy did not improve from 0.93238\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 190ms/step - accuracy: 0.9830 - loss: 0.0500 - val_accuracy: 0.9238 - val_loss: 0.4400 - learning_rate: 0.0010\n",
      "Epoch 18/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step - accuracy: 0.9838 - loss: 0.0438\n",
      "Epoch 18: val_accuracy did not improve from 0.93238\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 191ms/step - accuracy: 0.9842 - loss: 0.0441 - val_accuracy: 0.9305 - val_loss: 0.4236 - learning_rate: 0.0010\n",
      "Epoch 19/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 186ms/step - accuracy: 0.9858 - loss: 0.0411\n",
      "Epoch 19: val_accuracy improved from 0.93238 to 0.93381, saving model to models/age.keras\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 196ms/step - accuracy: 0.9857 - loss: 0.0395 - val_accuracy: 0.9338 - val_loss: 0.4706 - learning_rate: 0.0010\n",
      "Epoch 20/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.9868 - loss: 0.0382\n",
      "Epoch 20: val_accuracy did not improve from 0.93381\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 147ms/step - accuracy: 0.9866 - loss: 0.0409 - val_accuracy: 0.9338 - val_loss: 0.4711 - learning_rate: 0.0010\n",
      "Epoch 21/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - accuracy: 0.9884 - loss: 0.0331\n",
      "Epoch 21: val_accuracy improved from 0.93381 to 0.93476, saving model to models/age.keras\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 113ms/step - accuracy: 0.9879 - loss: 0.0350 - val_accuracy: 0.9348 - val_loss: 0.4378 - learning_rate: 0.0010\n",
      "Epoch 22/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9856 - loss: 0.0391\n",
      "Epoch 22: val_accuracy improved from 0.93476 to 0.93905, saving model to models/age.keras\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 111ms/step - accuracy: 0.9861 - loss: 0.0391 - val_accuracy: 0.9390 - val_loss: 0.5188 - learning_rate: 0.0010\n",
      "Epoch 23/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9849 - loss: 0.0434\n",
      "Epoch 23: val_accuracy did not improve from 0.93905\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 110ms/step - accuracy: 0.9858 - loss: 0.0404 - val_accuracy: 0.9371 - val_loss: 0.5092 - learning_rate: 0.0010\n",
      "Epoch 24/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9883 - loss: 0.0322\n",
      "Epoch 24: val_accuracy did not improve from 0.93905\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 110ms/step - accuracy: 0.9876 - loss: 0.0348 - val_accuracy: 0.9338 - val_loss: 0.5014 - learning_rate: 0.0010\n",
      "Epoch 25/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9880 - loss: 0.0353\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 0.0007000000332482159.\n",
      "\n",
      "Epoch 25: val_accuracy did not improve from 0.93905\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 109ms/step - accuracy: 0.9871 - loss: 0.0373 - val_accuracy: 0.9329 - val_loss: 0.5065 - learning_rate: 0.0010\n",
      "Epoch 26/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9936 - loss: 0.0202\n",
      "Epoch 26: val_accuracy did not improve from 0.93905\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 110ms/step - accuracy: 0.9935 - loss: 0.0196 - val_accuracy: 0.9390 - val_loss: 0.5160 - learning_rate: 7.0000e-04\n",
      "Epoch 27/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9966 - loss: 0.0108\n",
      "Epoch 27: val_accuracy did not improve from 0.93905\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 109ms/step - accuracy: 0.9962 - loss: 0.0114 - val_accuracy: 0.9362 - val_loss: 0.5515 - learning_rate: 7.0000e-04\n",
      "Epoch 28/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - accuracy: 0.9956 - loss: 0.0110\n",
      "Epoch 28: val_accuracy improved from 0.93905 to 0.93952, saving model to models/age.keras\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 145ms/step - accuracy: 0.9950 - loss: 0.0126 - val_accuracy: 0.9395 - val_loss: 0.6323 - learning_rate: 7.0000e-04\n",
      "Epoch 29/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step - accuracy: 0.9948 - loss: 0.0162\n",
      "Epoch 29: val_accuracy did not improve from 0.93952\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 191ms/step - accuracy: 0.9949 - loss: 0.0156 - val_accuracy: 0.9314 - val_loss: 0.5656 - learning_rate: 7.0000e-04\n",
      "Epoch 30/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step - accuracy: 0.9938 - loss: 0.0185\n",
      "Epoch 30: val_accuracy did not improve from 0.93952\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 189ms/step - accuracy: 0.9925 - loss: 0.0214 - val_accuracy: 0.9305 - val_loss: 0.5513 - learning_rate: 7.0000e-04\n",
      "Epoch 31/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step - accuracy: 0.9936 - loss: 0.0173\n",
      "Epoch 31: ReduceLROnPlateau reducing learning rate to 0.0004900000232737511.\n",
      "\n",
      "Epoch 31: val_accuracy did not improve from 0.93952\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 190ms/step - accuracy: 0.9934 - loss: 0.0180 - val_accuracy: 0.9371 - val_loss: 0.6171 - learning_rate: 7.0000e-04\n",
      "Epoch 32/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step - accuracy: 0.9967 - loss: 0.0115\n",
      "Epoch 32: val_accuracy did not improve from 0.93952\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 191ms/step - accuracy: 0.9960 - loss: 0.0112 - val_accuracy: 0.9390 - val_loss: 0.6184 - learning_rate: 4.9000e-04\n",
      "Epoch 33/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step - accuracy: 0.9983 - loss: 0.0055\n",
      "Epoch 33: val_accuracy did not improve from 0.93952\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 190ms/step - accuracy: 0.9982 - loss: 0.0056 - val_accuracy: 0.9386 - val_loss: 0.6915 - learning_rate: 4.9000e-04\n",
      "Epoch 34/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step - accuracy: 0.9979 - loss: 0.0049\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 0.00034300000406801696.\n",
      "\n",
      "Epoch 34: val_accuracy did not improve from 0.93952\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 190ms/step - accuracy: 0.9978 - loss: 0.0063 - val_accuracy: 0.9348 - val_loss: 0.6629 - learning_rate: 4.9000e-04\n",
      "Epoch 35/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step - accuracy: 0.9984 - loss: 0.0047\n",
      "Epoch 35: val_accuracy did not improve from 0.93952\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 190ms/step - accuracy: 0.9984 - loss: 0.0043 - val_accuracy: 0.9348 - val_loss: 0.6612 - learning_rate: 3.4300e-04\n",
      "Epoch 36/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step - accuracy: 0.9986 - loss: 0.0049\n",
      "Epoch 36: val_accuracy did not improve from 0.93952\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 190ms/step - accuracy: 0.9988 - loss: 0.0040 - val_accuracy: 0.9371 - val_loss: 0.7234 - learning_rate: 3.4300e-04\n",
      "Epoch 37/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step - accuracy: 0.9988 - loss: 0.0043\n",
      "Epoch 37: ReduceLROnPlateau reducing learning rate to 0.00024009999469853935.\n",
      "\n",
      "Epoch 37: val_accuracy did not improve from 0.93952\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 190ms/step - accuracy: 0.9982 - loss: 0.0050 - val_accuracy: 0.9352 - val_loss: 0.7656 - learning_rate: 3.4300e-04\n",
      "Epoch 38/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step - accuracy: 0.9986 - loss: 0.0039\n",
      "Epoch 38: val_accuracy did not improve from 0.93952\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 189ms/step - accuracy: 0.9988 - loss: 0.0041 - val_accuracy: 0.9362 - val_loss: 0.7345 - learning_rate: 2.4010e-04\n",
      "Epoch 38: early stopping\n",
      "Restoring model weights from the end of the best epoch: 28.\n",
      "Saved age\n",
      "\n",
      " Training gender (CNN)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\masked_face_emotion_project_light\\.venv\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step - accuracy: 0.8034 - loss: 0.4819\n",
      "Epoch 1: val_accuracy improved from None to 0.83048, saving model to models/gender.keras\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 194ms/step - accuracy: 0.8108 - loss: 0.4306 - val_accuracy: 0.8305 - val_loss: 0.3754 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.8282 - loss: 0.3655\n",
      "Epoch 2: val_accuracy improved from 0.83048 to 0.86524, saving model to models/gender.keras\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 122ms/step - accuracy: 0.8387 - loss: 0.3457 - val_accuracy: 0.8652 - val_loss: 0.3039 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.8705 - loss: 0.2870\n",
      "Epoch 3: val_accuracy improved from 0.86524 to 0.88429, saving model to models/gender.keras\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 110ms/step - accuracy: 0.8767 - loss: 0.2726 - val_accuracy: 0.8843 - val_loss: 0.2865 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9124 - loss: 0.2154\n",
      "Epoch 4: val_accuracy improved from 0.88429 to 0.90619, saving model to models/gender.keras\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 110ms/step - accuracy: 0.9123 - loss: 0.2093 - val_accuracy: 0.9062 - val_loss: 0.2367 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9306 - loss: 0.1705\n",
      "Epoch 5: val_accuracy improved from 0.90619 to 0.92857, saving model to models/gender.keras\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 111ms/step - accuracy: 0.9361 - loss: 0.1606 - val_accuracy: 0.9286 - val_loss: 0.1852 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9507 - loss: 0.1263\n",
      "Epoch 6: val_accuracy improved from 0.92857 to 0.95667, saving model to models/gender.keras\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 111ms/step - accuracy: 0.9545 - loss: 0.1198 - val_accuracy: 0.9567 - val_loss: 0.1474 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9670 - loss: 0.0867\n",
      "Epoch 7: val_accuracy did not improve from 0.95667\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 110ms/step - accuracy: 0.9676 - loss: 0.0860 - val_accuracy: 0.9519 - val_loss: 0.1403 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9765 - loss: 0.0668\n",
      "Epoch 8: val_accuracy improved from 0.95667 to 0.96095, saving model to models/gender.keras\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 111ms/step - accuracy: 0.9765 - loss: 0.0654 - val_accuracy: 0.9610 - val_loss: 0.1388 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9838 - loss: 0.0485\n",
      "Epoch 9: val_accuracy did not improve from 0.96095\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 111ms/step - accuracy: 0.9824 - loss: 0.0505 - val_accuracy: 0.9567 - val_loss: 0.1464 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9890 - loss: 0.0349\n",
      "Epoch 10: val_accuracy did not improve from 0.96095\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 111ms/step - accuracy: 0.9877 - loss: 0.0392 - val_accuracy: 0.9610 - val_loss: 0.1647 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy: 0.9900 - loss: 0.0278\n",
      "Epoch 11: val_accuracy improved from 0.96095 to 0.96524, saving model to models/gender.keras\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 176ms/step - accuracy: 0.9897 - loss: 0.0289 - val_accuracy: 0.9652 - val_loss: 0.1340 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step - accuracy: 0.9917 - loss: 0.0245\n",
      "Epoch 12: val_accuracy improved from 0.96524 to 0.96810, saving model to models/gender.keras\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 194ms/step - accuracy: 0.9929 - loss: 0.0219 - val_accuracy: 0.9681 - val_loss: 0.1399 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - accuracy: 0.9945 - loss: 0.0175\n",
      "Epoch 13: val_accuracy improved from 0.96810 to 0.97000, saving model to models/gender.keras\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 154ms/step - accuracy: 0.9915 - loss: 0.0239 - val_accuracy: 0.9700 - val_loss: 0.1778 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.9912 - loss: 0.0245\n",
      "Epoch 14: val_accuracy improved from 0.97000 to 0.97048, saving model to models/gender.keras\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 112ms/step - accuracy: 0.9893 - loss: 0.0291 - val_accuracy: 0.9705 - val_loss: 0.1409 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9927 - loss: 0.0191\n",
      "Epoch 15: val_accuracy improved from 0.97048 to 0.97381, saving model to models/gender.keras\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 112ms/step - accuracy: 0.9930 - loss: 0.0185 - val_accuracy: 0.9738 - val_loss: 0.1765 - learning_rate: 0.0010\n",
      "Epoch 16/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9953 - loss: 0.0153\n",
      "Epoch 16: val_accuracy did not improve from 0.97381\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 112ms/step - accuracy: 0.9942 - loss: 0.0172 - val_accuracy: 0.9676 - val_loss: 0.1700 - learning_rate: 0.0010\n",
      "Epoch 17/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.9944 - loss: 0.0145\n",
      "Epoch 17: val_accuracy did not improve from 0.97381\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 114ms/step - accuracy: 0.9942 - loss: 0.0162 - val_accuracy: 0.9676 - val_loss: 0.1685 - learning_rate: 0.0010\n",
      "Epoch 18/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9951 - loss: 0.0138\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.0007000000332482159.\n",
      "\n",
      "Epoch 18: val_accuracy did not improve from 0.97381\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 111ms/step - accuracy: 0.9924 - loss: 0.0220 - val_accuracy: 0.9686 - val_loss: 0.1828 - learning_rate: 0.0010\n",
      "Epoch 19/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9946 - loss: 0.0173\n",
      "Epoch 19: val_accuracy did not improve from 0.97381\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 111ms/step - accuracy: 0.9964 - loss: 0.0126 - val_accuracy: 0.9714 - val_loss: 0.1923 - learning_rate: 7.0000e-04\n",
      "Epoch 20/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9992 - loss: 0.0035\n",
      "Epoch 20: val_accuracy did not improve from 0.97381\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 111ms/step - accuracy: 0.9990 - loss: 0.0036 - val_accuracy: 0.9738 - val_loss: 0.1806 - learning_rate: 7.0000e-04\n",
      "Epoch 21/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 0.9996 - loss: 0.0020\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 0.0004900000232737511.\n",
      "\n",
      "Epoch 21: val_accuracy did not improve from 0.97381\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 138ms/step - accuracy: 0.9987 - loss: 0.0043 - val_accuracy: 0.9676 - val_loss: 0.2047 - learning_rate: 7.0000e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.9980 - loss: 0.0066\n",
      "Epoch 22: val_accuracy did not improve from 0.97381\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 144ms/step - accuracy: 0.9987 - loss: 0.0046 - val_accuracy: 0.9719 - val_loss: 0.2168 - learning_rate: 4.9000e-04\n",
      "Epoch 23/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9997 - loss: 0.0022\n",
      "Epoch 23: val_accuracy did not improve from 0.97381\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 111ms/step - accuracy: 0.9996 - loss: 0.0026 - val_accuracy: 0.9714 - val_loss: 0.2229 - learning_rate: 4.9000e-04\n",
      "Epoch 24/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 411ms/step - accuracy: 1.0000 - loss: 8.7765e-04\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 0.00034300000406801696.\n",
      "\n",
      "Epoch 24: val_accuracy did not improve from 0.97381\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 415ms/step - accuracy: 0.9998 - loss: 0.0011 - val_accuracy: 0.9729 - val_loss: 0.2554 - learning_rate: 4.9000e-04\n",
      "Epoch 25/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - accuracy: 0.9998 - loss: 8.6707e-04\n",
      "Epoch 25: val_accuracy did not improve from 0.97381\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 118ms/step - accuracy: 0.9997 - loss: 0.0012 - val_accuracy: 0.9729 - val_loss: 0.2515 - learning_rate: 3.4300e-04\n",
      "Epoch 25: early stopping\n",
      "Restoring model weights from the end of the best epoch: 15.\n",
      "Saved gender\n",
      "\n",
      " Training ethnicity (CNN)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\masked_face_emotion_project_light\\.venv\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy: 0.7400 - loss: 0.8435\n",
      "Epoch 1: val_accuracy improved from None to 0.75524, saving model to models/ethnicity.keras\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 177ms/step - accuracy: 0.7527 - loss: 0.8011 - val_accuracy: 0.7552 - val_loss: 0.7254 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.7490 - loss: 0.7330\n",
      "Epoch 2: val_accuracy improved from 0.75524 to 0.77286, saving model to models/ethnicity.keras\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 131ms/step - accuracy: 0.7583 - loss: 0.7005 - val_accuracy: 0.7729 - val_loss: 0.6289 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.7671 - loss: 0.6351\n",
      "Epoch 3: val_accuracy improved from 0.77286 to 0.79524, saving model to models/ethnicity.keras\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 112ms/step - accuracy: 0.7745 - loss: 0.6092 - val_accuracy: 0.7952 - val_loss: 0.5545 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.8034 - loss: 0.5134\n",
      "Epoch 4: val_accuracy improved from 0.79524 to 0.82762, saving model to models/ethnicity.keras\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 111ms/step - accuracy: 0.8073 - loss: 0.5050 - val_accuracy: 0.8276 - val_loss: 0.4544 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.8376 - loss: 0.4208\n",
      "Epoch 5: val_accuracy improved from 0.82762 to 0.84381, saving model to models/ethnicity.keras\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 112ms/step - accuracy: 0.8428 - loss: 0.4112 - val_accuracy: 0.8438 - val_loss: 0.4328 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.8747 - loss: 0.3326\n",
      "Epoch 6: val_accuracy improved from 0.84381 to 0.89238, saving model to models/ethnicity.keras\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 111ms/step - accuracy: 0.8782 - loss: 0.3250 - val_accuracy: 0.8924 - val_loss: 0.3103 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.9036 - loss: 0.2537\n",
      "Epoch 7: val_accuracy improved from 0.89238 to 0.91000, saving model to models/ethnicity.keras\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 112ms/step - accuracy: 0.9033 - loss: 0.2542 - val_accuracy: 0.9100 - val_loss: 0.2830 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.9268 - loss: 0.1935\n",
      "Epoch 8: val_accuracy improved from 0.91000 to 0.92381, saving model to models/ethnicity.keras\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 112ms/step - accuracy: 0.9295 - loss: 0.1953 - val_accuracy: 0.9238 - val_loss: 0.2519 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9428 - loss: 0.1549\n",
      "Epoch 9: val_accuracy improved from 0.92381 to 0.93905, saving model to models/ethnicity.keras\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 111ms/step - accuracy: 0.9434 - loss: 0.1531 - val_accuracy: 0.9390 - val_loss: 0.2247 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.9548 - loss: 0.1242\n",
      "Epoch 10: val_accuracy improved from 0.93905 to 0.94524, saving model to models/ethnicity.keras\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 113ms/step - accuracy: 0.9549 - loss: 0.1225 - val_accuracy: 0.9452 - val_loss: 0.2113 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.9663 - loss: 0.0927\n",
      "Epoch 11: val_accuracy did not improve from 0.94524\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 111ms/step - accuracy: 0.9626 - loss: 0.1019 - val_accuracy: 0.9405 - val_loss: 0.2343 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.9686 - loss: 0.0877\n",
      "Epoch 12: val_accuracy improved from 0.94524 to 0.95048, saving model to models/ethnicity.keras\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 112ms/step - accuracy: 0.9667 - loss: 0.0895 - val_accuracy: 0.9505 - val_loss: 0.2430 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.9734 - loss: 0.0717\n",
      "Epoch 13: val_accuracy improved from 0.95048 to 0.95571, saving model to models/ethnicity.keras\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 116ms/step - accuracy: 0.9725 - loss: 0.0756 - val_accuracy: 0.9557 - val_loss: 0.2366 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9788 - loss: 0.0589\n",
      "Epoch 14: val_accuracy improved from 0.95571 to 0.95667, saving model to models/ethnicity.keras\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 111ms/step - accuracy: 0.9778 - loss: 0.0609 - val_accuracy: 0.9567 - val_loss: 0.2381 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9830 - loss: 0.0523\n",
      "Epoch 15: val_accuracy improved from 0.95667 to 0.95810, saving model to models/ethnicity.keras\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 111ms/step - accuracy: 0.9804 - loss: 0.0555 - val_accuracy: 0.9581 - val_loss: 0.2637 - learning_rate: 0.0010\n",
      "Epoch 16/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9820 - loss: 0.0491\n",
      "Epoch 16: val_accuracy did not improve from 0.95810\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 111ms/step - accuracy: 0.9805 - loss: 0.0526 - val_accuracy: 0.9543 - val_loss: 0.2572 - learning_rate: 0.0010\n",
      "Epoch 17/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9839 - loss: 0.0459\n",
      "Epoch 17: val_accuracy did not improve from 0.95810\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 111ms/step - accuracy: 0.9840 - loss: 0.0473 - val_accuracy: 0.9571 - val_loss: 0.3011 - learning_rate: 0.0010\n",
      "Epoch 18/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9859 - loss: 0.0373\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.0007000000332482159.\n",
      "\n",
      "Epoch 18: val_accuracy did not improve from 0.95810\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 110ms/step - accuracy: 0.9837 - loss: 0.0449 - val_accuracy: 0.9543 - val_loss: 0.2552 - learning_rate: 0.0010\n",
      "Epoch 19/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9893 - loss: 0.0322\n",
      "Epoch 19: val_accuracy improved from 0.95810 to 0.96048, saving model to models/ethnicity.keras\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 111ms/step - accuracy: 0.9908 - loss: 0.0289 - val_accuracy: 0.9605 - val_loss: 0.2833 - learning_rate: 7.0000e-04\n",
      "Epoch 20/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9929 - loss: 0.0202\n",
      "Epoch 20: val_accuracy improved from 0.96048 to 0.96476, saving model to models/ethnicity.keras\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 111ms/step - accuracy: 0.9922 - loss: 0.0216 - val_accuracy: 0.9648 - val_loss: 0.2980 - learning_rate: 7.0000e-04\n",
      "Epoch 21/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.9926 - loss: 0.0210\n",
      "Epoch 21: val_accuracy did not improve from 0.96476\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 111ms/step - accuracy: 0.9916 - loss: 0.0223 - val_accuracy: 0.9614 - val_loss: 0.3025 - learning_rate: 7.0000e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.9945 - loss: 0.0170\n",
      "Epoch 22: val_accuracy did not improve from 0.96476\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 160ms/step - accuracy: 0.9932 - loss: 0.0204 - val_accuracy: 0.9614 - val_loss: 0.2956 - learning_rate: 7.0000e-04\n",
      "Epoch 23/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9926 - loss: 0.0216\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 0.0004900000232737511.\n",
      "\n",
      "Epoch 23: val_accuracy did not improve from 0.96476\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 110ms/step - accuracy: 0.9918 - loss: 0.0245 - val_accuracy: 0.9619 - val_loss: 0.3263 - learning_rate: 7.0000e-04\n",
      "Epoch 24/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9948 - loss: 0.0157\n",
      "Epoch 24: val_accuracy did not improve from 0.96476\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 110ms/step - accuracy: 0.9948 - loss: 0.0157 - val_accuracy: 0.9633 - val_loss: 0.3297 - learning_rate: 4.9000e-04\n",
      "Epoch 25/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9942 - loss: 0.0165\n",
      "Epoch 25: val_accuracy did not improve from 0.96476\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 110ms/step - accuracy: 0.9946 - loss: 0.0152 - val_accuracy: 0.9624 - val_loss: 0.3662 - learning_rate: 4.9000e-04\n",
      "Epoch 26/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9956 - loss: 0.0138\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 0.00034300000406801696.\n",
      "\n",
      "Epoch 26: val_accuracy did not improve from 0.96476\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 111ms/step - accuracy: 0.9958 - loss: 0.0135 - val_accuracy: 0.9629 - val_loss: 0.3228 - learning_rate: 4.9000e-04\n",
      "Epoch 27/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9971 - loss: 0.0100\n",
      "Epoch 27: val_accuracy did not improve from 0.96476\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 110ms/step - accuracy: 0.9973 - loss: 0.0096 - val_accuracy: 0.9643 - val_loss: 0.3607 - learning_rate: 3.4300e-04\n",
      "Epoch 28/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9978 - loss: 0.0071\n",
      "Epoch 28: val_accuracy did not improve from 0.96476\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 111ms/step - accuracy: 0.9978 - loss: 0.0072 - val_accuracy: 0.9619 - val_loss: 0.3659 - learning_rate: 3.4300e-04\n",
      "Epoch 29/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9973 - loss: 0.0082\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 0.00024009999469853935.\n",
      "\n",
      "Epoch 29: val_accuracy did not improve from 0.96476\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 111ms/step - accuracy: 0.9980 - loss: 0.0071 - val_accuracy: 0.9638 - val_loss: 0.3993 - learning_rate: 3.4300e-04\n",
      "Epoch 30/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9976 - loss: 0.0064\n",
      "Epoch 30: val_accuracy did not improve from 0.96476\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 110ms/step - accuracy: 0.9979 - loss: 0.0060 - val_accuracy: 0.9648 - val_loss: 0.3986 - learning_rate: 2.4010e-04\n",
      "Epoch 30: early stopping\n",
      "Restoring model weights from the end of the best epoch: 20.\n",
      "Saved ethnicity\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# CNN Training for All Tasks (50 epochs each)\n",
    "# =========================\n",
    "\n",
    "# Load dataset (update with your CSV path)\n",
    "df = pd.read_csv(\"dataset/masked_face_dataset.csv\")\n",
    "\n",
    "tasks = {\n",
    "    \"emotion\": \"emotion_label\",\n",
    "    \"age\": \"age_range\",\n",
    "    \"gender\": \"gender\",\n",
    "    \"ethnicity\": \"ethnicity\"\n",
    "}\n",
    "\n",
    "for task, label_col in tasks.items():\n",
    "    print(f\"\\n Training {task} (CNN)\")\n",
    "    (X_train, y_train), (X_val, y_val), (X_test, y_test), classes = prepare_data(df, label_col, img_size=(64,64))\n",
    "    cnn = build_cnn(len(classes), input_shape=(64,64,3))\n",
    "    train_and_save(cnn, X_train, y_train, X_val, y_val, f\"{task}\", classes, epochs=50, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2e2f991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training emotion (Transformer)\n",
      " Falling back to random initialization: Shape mismatch in layer #1 (named stem_conv)for weight stem_conv/kernel. Weight expects shape (3, 3, 1, 32). Received saved weight with shape (3, 3, 3, 32)\n",
      "Epoch 1/50\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 0.1473 - loss: 2.0208\n",
      "Epoch 1: val_accuracy improved from None to 0.14286, saving model to models/emotion_transformer.keras\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 181ms/step - accuracy: 0.1432 - loss: 1.9834 - val_accuracy: 0.1429 - val_loss: 1.9470 - learning_rate: 1.0000e-04\n",
      "Epoch 2/50\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 0.1482 - loss: 1.9494\n",
      "Epoch 2: val_accuracy improved from 0.14286 to 0.14571, saving model to models/emotion_transformer.keras\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 175ms/step - accuracy: 0.1444 - loss: 1.9488 - val_accuracy: 0.1457 - val_loss: 1.9477 - learning_rate: 1.0000e-04\n",
      "Epoch 3/50\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 0.1393 - loss: 1.9462\n",
      "Epoch 3: val_accuracy did not improve from 0.14571\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 173ms/step - accuracy: 0.1393 - loss: 1.9461 - val_accuracy: 0.1429 - val_loss: 1.9466 - learning_rate: 1.0000e-04\n",
      "Epoch 4/50\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy: 0.1478 - loss: 1.9461\n",
      "Epoch 4: val_accuracy did not improve from 0.14571\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 179ms/step - accuracy: 0.1460 - loss: 1.9463 - val_accuracy: 0.1429 - val_loss: 1.9462 - learning_rate: 1.0000e-04\n",
      "Epoch 5/50\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 0.1435 - loss: 1.9460\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 6.999999823165126e-05.\n",
      "\n",
      "Epoch 5: val_accuracy did not improve from 0.14571\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 179ms/step - accuracy: 0.1435 - loss: 1.9461 - val_accuracy: 0.1429 - val_loss: 1.9470 - learning_rate: 1.0000e-04\n",
      "Epoch 6/50\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step - accuracy: 0.1365 - loss: 1.9460\n",
      "Epoch 6: val_accuracy did not improve from 0.14571\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 188ms/step - accuracy: 0.1426 - loss: 1.9460 - val_accuracy: 0.1429 - val_loss: 1.9467 - learning_rate: 7.0000e-05\n",
      "Epoch 7/50\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - accuracy: 0.1431 - loss: 1.9459\n",
      "Epoch 7: val_accuracy did not improve from 0.14571\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 188ms/step - accuracy: 0.1429 - loss: 1.9459 - val_accuracy: 0.1429 - val_loss: 1.9467 - learning_rate: 7.0000e-05\n",
      "Epoch 8/50\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172ms/step - accuracy: 0.1428 - loss: 1.9459\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 4.899999621557071e-05.\n",
      "\n",
      "Epoch 8: val_accuracy did not improve from 0.14571\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 189ms/step - accuracy: 0.1429 - loss: 1.9459 - val_accuracy: 0.1429 - val_loss: 1.9467 - learning_rate: 7.0000e-05\n",
      "Epoch 9/50\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174ms/step - accuracy: 0.1395 - loss: 1.9460\n",
      "Epoch 9: val_accuracy did not improve from 0.14571\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 192ms/step - accuracy: 0.1421 - loss: 1.9461 - val_accuracy: 0.1419 - val_loss: 1.9464 - learning_rate: 4.9000e-05\n",
      "Epoch 10/50\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 175ms/step - accuracy: 0.1439 - loss: 1.9460\n",
      "Epoch 10: val_accuracy did not improve from 0.14571\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 192ms/step - accuracy: 0.1429 - loss: 1.9460 - val_accuracy: 0.1429 - val_loss: 1.9467 - learning_rate: 4.9000e-05\n",
      "Epoch 11/50\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 175ms/step - accuracy: 0.1402 - loss: 1.9459\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 3.4299996332265434e-05.\n",
      "\n",
      "Epoch 11: val_accuracy did not improve from 0.14571\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 192ms/step - accuracy: 0.1429 - loss: 1.9459 - val_accuracy: 0.1429 - val_loss: 1.9467 - learning_rate: 4.9000e-05\n",
      "Epoch 12/50\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 175ms/step - accuracy: 0.1477 - loss: 1.9460\n",
      "Epoch 12: val_accuracy did not improve from 0.14571\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 192ms/step - accuracy: 0.1420 - loss: 1.9460 - val_accuracy: 0.1429 - val_loss: 1.9473 - learning_rate: 3.4300e-05\n",
      "Epoch 12: early stopping\n",
      "Restoring model weights from the end of the best epoch: 2.\n",
      "Saved emotion_transformer\n",
      "\n",
      " Training age (Transformer)\n",
      " Falling back to random initialization: Shape mismatch in layer #1 (named stem_conv)for weight stem_conv/kernel. Weight expects shape (3, 3, 1, 32). Received saved weight with shape (3, 3, 3, 32)\n",
      "Epoch 1/50\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 189ms/step - accuracy: 0.5536 - loss: 0.8860\n",
      "Epoch 1: val_accuracy improved from None to 0.59762, saving model to models/age_transformer.keras\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 211ms/step - accuracy: 0.5660 - loss: 0.8389 - val_accuracy: 0.5976 - val_loss: 1.2218 - learning_rate: 1.0000e-04\n",
      "Epoch 2/50\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step - accuracy: 0.5763 - loss: 0.8061\n",
      "Epoch 2: val_accuracy did not improve from 0.59762\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 187ms/step - accuracy: 0.5854 - loss: 0.7963 - val_accuracy: 0.3771 - val_loss: 1.0189 - learning_rate: 1.0000e-04\n",
      "Epoch 3/50\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173ms/step - accuracy: 0.5893 - loss: 0.7886\n",
      "Epoch 3: val_accuracy did not improve from 0.59762\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 190ms/step - accuracy: 0.5938 - loss: 0.7851 - val_accuracy: 0.3771 - val_loss: 1.5573 - learning_rate: 1.0000e-04\n",
      "Epoch 4/50\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172ms/step - accuracy: 0.6028 - loss: 0.7782\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 6.999999823165126e-05.\n",
      "\n",
      "Epoch 4: val_accuracy did not improve from 0.59762\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 189ms/step - accuracy: 0.6135 - loss: 0.7638 - val_accuracy: 0.5976 - val_loss: 1.4596 - learning_rate: 1.0000e-04\n",
      "Epoch 5/50\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - accuracy: 0.6283 - loss: 0.7461\n",
      "Epoch 5: val_accuracy did not improve from 0.59762\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 189ms/step - accuracy: 0.6278 - loss: 0.7455 - val_accuracy: 0.5976 - val_loss: 0.9273 - learning_rate: 7.0000e-05\n",
      "Epoch 6/50\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 177ms/step - accuracy: 0.6301 - loss: 0.7363\n",
      "Epoch 6: val_accuracy did not improve from 0.59762\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 195ms/step - accuracy: 0.6335 - loss: 0.7384 - val_accuracy: 0.5976 - val_loss: 2.6931 - learning_rate: 7.0000e-05\n",
      "Epoch 7/50\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174ms/step - accuracy: 0.6304 - loss: 0.7391\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 4.899999621557071e-05.\n",
      "\n",
      "Epoch 7: val_accuracy did not improve from 0.59762\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 192ms/step - accuracy: 0.6288 - loss: 0.7390 - val_accuracy: 0.5976 - val_loss: 4.8684 - learning_rate: 7.0000e-05\n",
      "Epoch 8/50\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 194ms/step - accuracy: 0.6373 - loss: 0.7341\n",
      "Epoch 8: val_accuracy did not improve from 0.59762\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 213ms/step - accuracy: 0.6343 - loss: 0.7323 - val_accuracy: 0.5976 - val_loss: 1.0231 - learning_rate: 4.9000e-05\n",
      "Epoch 9/50\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 176ms/step - accuracy: 0.6389 - loss: 0.7282\n",
      "Epoch 9: val_accuracy did not improve from 0.59762\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 193ms/step - accuracy: 0.6344 - loss: 0.7321 - val_accuracy: 0.5976 - val_loss: 9.9336 - learning_rate: 4.9000e-05\n",
      "Epoch 10/50\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step - accuracy: 0.6364 - loss: 0.7400\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 3.4299996332265434e-05.\n",
      "\n",
      "Epoch 10: val_accuracy did not improve from 0.59762\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 187ms/step - accuracy: 0.6336 - loss: 0.7329 - val_accuracy: 0.5976 - val_loss: 0.8414 - learning_rate: 4.9000e-05\n",
      "Epoch 11/50\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173ms/step - accuracy: 0.6344 - loss: 0.7343\n",
      "Epoch 11: val_accuracy did not improve from 0.59762\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 190ms/step - accuracy: 0.6382 - loss: 0.7288 - val_accuracy: 0.5976 - val_loss: 7.8717 - learning_rate: 3.4300e-05\n",
      "Epoch 11: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Saved age_transformer\n",
      "\n",
      " Training gender (Transformer)\n",
      " Falling back to random initialization: Shape mismatch in layer #1 (named stem_conv)for weight stem_conv/kernel. Weight expects shape (3, 3, 1, 32). Received saved weight with shape (3, 3, 3, 32)\n",
      "Epoch 1/50\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step - accuracy: 0.7859 - loss: 0.5596\n",
      "Epoch 1: val_accuracy improved from None to 0.80429, saving model to models/gender_transformer.keras\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 209ms/step - accuracy: 0.7973 - loss: 0.5327 - val_accuracy: 0.8043 - val_loss: 0.5573 - learning_rate: 1.0000e-04\n",
      "Epoch 2/50\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 178ms/step - accuracy: 0.8028 - loss: 0.5084\n",
      "Epoch 2: val_accuracy did not improve from 0.80429\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 196ms/step - accuracy: 0.8042 - loss: 0.5058 - val_accuracy: 0.8043 - val_loss: 0.5114 - learning_rate: 1.0000e-04\n",
      "Epoch 3/50\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step - accuracy: 0.8058 - loss: 0.5000\n",
      "Epoch 3: val_accuracy did not improve from 0.80429\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 202ms/step - accuracy: 0.8045 - loss: 0.5007 - val_accuracy: 0.8043 - val_loss: 0.5573 - learning_rate: 1.0000e-04\n",
      "Epoch 4/50\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 186ms/step - accuracy: 0.8040 - loss: 0.4883\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 6.999999823165126e-05.\n",
      "\n",
      "Epoch 4: val_accuracy did not improve from 0.80429\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 204ms/step - accuracy: 0.8042 - loss: 0.4818 - val_accuracy: 0.2605 - val_loss: 0.7069 - learning_rate: 1.0000e-04\n",
      "Epoch 5/50\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179ms/step - accuracy: 0.8060 - loss: 0.4635\n",
      "Epoch 5: val_accuracy did not improve from 0.80429\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 197ms/step - accuracy: 0.8036 - loss: 0.4614 - val_accuracy: 0.8043 - val_loss: 3.9442 - learning_rate: 7.0000e-05\n",
      "Epoch 6/50\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 177ms/step - accuracy: 0.8072 - loss: 0.4470\n",
      "Epoch 6: val_accuracy did not improve from 0.80429\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 195ms/step - accuracy: 0.8043 - loss: 0.4510 - val_accuracy: 0.1957 - val_loss: 0.7886 - learning_rate: 7.0000e-05\n",
      "Epoch 7/50\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 176ms/step - accuracy: 0.7999 - loss: 0.4543\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 4.899999621557071e-05.\n",
      "\n",
      "Epoch 7: val_accuracy did not improve from 0.80429\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 194ms/step - accuracy: 0.8044 - loss: 0.4504 - val_accuracy: 0.8043 - val_loss: 0.4612 - learning_rate: 7.0000e-05\n",
      "Epoch 8/50\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 176ms/step - accuracy: 0.8031 - loss: 0.4449\n",
      "Epoch 8: val_accuracy did not improve from 0.80429\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 193ms/step - accuracy: 0.8047 - loss: 0.4448 - val_accuracy: 0.8043 - val_loss: 4.2015 - learning_rate: 4.9000e-05\n",
      "Epoch 9/50\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 175ms/step - accuracy: 0.8048 - loss: 0.4386\n",
      "Epoch 9: val_accuracy did not improve from 0.80429\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 195ms/step - accuracy: 0.8044 - loss: 0.4410 - val_accuracy: 0.8043 - val_loss: 5.0591 - learning_rate: 4.9000e-05\n",
      "Epoch 10/50\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step - accuracy: 0.8039 - loss: 0.4485\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 3.4299996332265434e-05.\n",
      "\n",
      "Epoch 10: val_accuracy did not improve from 0.80429\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 197ms/step - accuracy: 0.8043 - loss: 0.4430 - val_accuracy: 0.8043 - val_loss: 0.6371 - learning_rate: 4.9000e-05\n",
      "Epoch 11/50\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173ms/step - accuracy: 0.8079 - loss: 0.4333\n",
      "Epoch 11: val_accuracy did not improve from 0.80429\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 193ms/step - accuracy: 0.8042 - loss: 0.4396 - val_accuracy: 0.8043 - val_loss: 3.2062 - learning_rate: 3.4300e-05\n",
      "Epoch 11: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Saved gender_transformer\n",
      "\n",
      " Training ethnicity (Transformer)\n",
      " Falling back to random initialization: Shape mismatch in layer #1 (named stem_conv)for weight stem_conv/kernel. Weight expects shape (3, 3, 1, 32). Received saved weight with shape (3, 3, 3, 32)\n",
      "Epoch 1/50\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 201ms/step - accuracy: 0.7270 - loss: 0.9705\n",
      "Epoch 1: val_accuracy improved from None to 0.75476, saving model to models/ethnicity_transformer.keras\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 230ms/step - accuracy: 0.7461 - loss: 0.9080 - val_accuracy: 0.7548 - val_loss: 1.3804 - learning_rate: 1.0000e-04\n",
      "Epoch 2/50\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 327ms/step - accuracy: 0.7554 - loss: 0.8604\n",
      "Epoch 2: val_accuracy did not improve from 0.75476\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m191s\u001b[0m 365ms/step - accuracy: 0.7545 - loss: 0.8574 - val_accuracy: 0.7548 - val_loss: 0.8266 - learning_rate: 1.0000e-04\n",
      "Epoch 3/50\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 213ms/step - accuracy: 0.7579 - loss: 0.8491\n",
      "Epoch 3: val_accuracy did not improve from 0.75476\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 229ms/step - accuracy: 0.7549 - loss: 0.8526 - val_accuracy: 0.7548 - val_loss: 0.8746 - learning_rate: 1.0000e-04\n",
      "Epoch 4/50\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 0.7501 - loss: 0.8512\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 6.999999823165126e-05.\n",
      "\n",
      "Epoch 4: val_accuracy did not improve from 0.75476\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 174ms/step - accuracy: 0.7551 - loss: 0.8453 - val_accuracy: 0.7548 - val_loss: 0.8601 - learning_rate: 1.0000e-04\n",
      "Epoch 5/50\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 0.7613 - loss: 0.8162\n",
      "Epoch 5: val_accuracy did not improve from 0.75476\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 174ms/step - accuracy: 0.7550 - loss: 0.8314 - val_accuracy: 0.7548 - val_loss: 1.2721 - learning_rate: 7.0000e-05\n",
      "Epoch 6/50\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 0.7560 - loss: 0.8208\n",
      "Epoch 6: val_accuracy did not improve from 0.75476\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 174ms/step - accuracy: 0.7551 - loss: 0.8244 - val_accuracy: 0.7548 - val_loss: 2.4638 - learning_rate: 7.0000e-05\n",
      "Epoch 7/50\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 0.7500 - loss: 0.8326\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 4.899999621557071e-05.\n",
      "\n",
      "Epoch 7: val_accuracy did not improve from 0.75476\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 175ms/step - accuracy: 0.7551 - loss: 0.8187 - val_accuracy: 0.7548 - val_loss: 1.4872 - learning_rate: 7.0000e-05\n",
      "Epoch 8/50\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step - accuracy: 0.7563 - loss: 0.8148\n",
      "Epoch 8: val_accuracy did not improve from 0.75476\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 187ms/step - accuracy: 0.7551 - loss: 0.8167 - val_accuracy: 0.7548 - val_loss: 0.8590 - learning_rate: 4.9000e-05\n",
      "Epoch 9/50\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 175ms/step - accuracy: 0.7474 - loss: 0.8370\n",
      "Epoch 9: val_accuracy did not improve from 0.75476\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 193ms/step - accuracy: 0.7550 - loss: 0.8154 - val_accuracy: 0.7548 - val_loss: 1.0436 - learning_rate: 4.9000e-05\n",
      "Epoch 10/50\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 177ms/step - accuracy: 0.7539 - loss: 0.8164\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 3.4299996332265434e-05.\n",
      "\n",
      "Epoch 10: val_accuracy did not improve from 0.75476\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 196ms/step - accuracy: 0.7551 - loss: 0.8101 - val_accuracy: 0.0624 - val_loss: 2.1424 - learning_rate: 4.9000e-05\n",
      "Epoch 11/50\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 177ms/step - accuracy: 0.7565 - loss: 0.8139\n",
      "Epoch 11: val_accuracy did not improve from 0.75476\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 194ms/step - accuracy: 0.7551 - loss: 0.8083 - val_accuracy: 0.7548 - val_loss: 1.0353 - learning_rate: 3.4300e-05\n",
      "Epoch 11: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Saved ethnicity_transformer\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Transformer Training for All Tasks (50 epochs each)\n",
    "# =========================\n",
    "\n",
    "for task, label_col in tasks.items():\n",
    "    print(f\"\\n Training {task} (Transformer)\")\n",
    "    (X_train, y_train), (X_val, y_val), (X_test, y_test), classes = prepare_data(df, label_col, img_size=(128,128))\n",
    "    transf = build_transformer(len(classes), input_shape=(128,128,3))\n",
    "    train_and_save(transf, X_train, y_train, X_val, y_val, f\"{task}_transformer\", classes, epochs=50, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80d186ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-01 19:42:06.573 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-01 19:42:06.574 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-01 19:42:07.002 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run d:\\masked_face_emotion_project_light\\.venv\\Lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n",
      "2025-09-01 19:42:07.003 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-01 19:42:07.004 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-01 19:42:07.005 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-01 19:42:07.006 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-01 19:42:07.006 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-01 19:42:07.007 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-01 19:42:07.008 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-01 19:42:07.008 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-01 19:42:08.197 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-01 19:42:08.198 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-01 19:42:08.199 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-01 19:42:08.201 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-01 19:42:08.202 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-01 19:42:08.203 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-01 19:42:08.205 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-01 19:42:08.205 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-01 19:42:08.206 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-01 19:42:08.207 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-01 19:42:08.207 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-01 19:42:08.208 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-01 19:42:08.210 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-01 19:42:08.210 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-01 19:42:08.211 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-01 19:42:08.212 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-01 19:42:08.212 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-01 19:42:08.213 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-01 19:42:08.214 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-01 19:42:08.214 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-01 19:42:08.215 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-01 19:42:08.215 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-01 19:42:08.218 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-01 19:42:08.220 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-01 19:42:08.221 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "import pickle\n",
    "from utils import preprocess_image\n",
    "\n",
    "st.set_page_config(page_title=\" Masked Face Recognition\", layout=\"wide\")\n",
    "\n",
    "st.markdown(\n",
    "    \"\"\"\n",
    "    <style>\n",
    "        .title {\n",
    "            font-size: 36px;\n",
    "            font-weight: bold;\n",
    "            text-align: center;\n",
    "            color: #4CAF50;\n",
    "        }\n",
    "        .subtitle {\n",
    "            text-align: center;\n",
    "            font-size: 18px;\n",
    "            color: #888;\n",
    "        }\n",
    "        .result-card {\n",
    "            padding: 20px;\n",
    "            border-radius: 15px;\n",
    "            background-color: #f8f9fa;\n",
    "            margin-bottom: 15px;\n",
    "            box-shadow: 2px 2px 8px rgba(0,0,0,0.1);\n",
    "        }\n",
    "    </style>\n",
    "    \"\"\",\n",
    "    unsafe_allow_html=True,\n",
    ")\n",
    "\n",
    "st.markdown('<div class=\"title\">Masked Face Attribute Recognition</div>', unsafe_allow_html=True)\n",
    "st.markdown('<div class=\"subtitle\">Predict Emotion, Age Range, Gender & Ethnicity</div>', unsafe_allow_html=True)\n",
    "\n",
    "\n",
    "def load_model_and_labels(model_path, labels_path):\n",
    "    try:\n",
    "        model = load_model(model_path)\n",
    "        with open(labels_path, \"rb\") as f:\n",
    "            labels = pickle.load(f)\n",
    "        return model, labels\n",
    "    except Exception as e:\n",
    "        st.error(f\"Error loading {model_path}: {e}\")\n",
    "        return None, []\n",
    "\n",
    "cnn_emotion, EMOTIONS   = load_model_and_labels(\"models/emotion.keras\", \"models/emotion_classes.pkl\")\n",
    "cnn_age, AGES           = load_model_and_labels(\"models/age.keras\", \"models/age_classes.pkl\")\n",
    "cnn_gender, GENDERS     = load_model_and_labels(\"models/gender.keras\", \"models/gender_classes.pkl\")\n",
    "cnn_ethnicity, ETHNICITY= load_model_and_labels(\"models/ethnicity.keras\", \"models/ethnicity_classes.pkl\")\n",
    "\n",
    "AGE_MAPPING = {\n",
    "    \"0\": \"0–9\", \"1\": \"10–19\", \"2\": \"20–29\", \"3\": \"30–39\",\n",
    "    \"4\": \"40–49\", \"5\": \"50–59\", \"6\": \"60–69\", \"7\": \"70+\"\n",
    "}\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "\n",
    "def predict_attributes(img):\n",
    "    img_processed = preprocess_image(img)  \n",
    "    img_processed = np.expand_dims(img_processed, axis=0)\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    if cnn_emotion:\n",
    "        pred = cnn_emotion.predict(img_processed, verbose=0)\n",
    "        results[\"Emotion\"] = (EMOTIONS[np.argmax(pred)], float(np.max(pred)))\n",
    "\n",
    "    if cnn_age:\n",
    "        pred = cnn_age.predict(img_processed, verbose=0)\n",
    "        raw_label = str(AGES[np.argmax(pred)])\n",
    "        results[\"Age Range\"] = (AGE_MAPPING.get(raw_label, raw_label), float(np.max(pred)))\n",
    "\n",
    "    if cnn_gender:\n",
    "        pred = cnn_gender.predict(img_processed, verbose=0)\n",
    "        results[\"Gender\"] = (GENDERS[np.argmax(pred)], float(np.max(pred)))\n",
    "\n",
    "    if cnn_ethnicity:\n",
    "        pred = cnn_ethnicity.predict(img_processed, verbose=0)\n",
    "        results[\"Ethnicity\"] = (ETHNICITY[np.argmax(pred)], float(np.max(pred)))\n",
    "\n",
    "    return results\n",
    "\n",
    "tab1, tab2 = st.tabs([\"Upload Image\", \"Live Webcam\"])\n",
    "\n",
    "# ===== Upload Tab =====\n",
    "with tab1:\n",
    "    uploaded_file = st.file_uploader(\"Upload a face image\", type=[\"jpg\", \"png\", \"jpeg\"])\n",
    "    if uploaded_file is not None:\n",
    "        file_name = uploaded_file.name.lower() \n",
    "        file_bytes = np.frombuffer(uploaded_file.read(), np.uint8)\n",
    "        img = cv2.imdecode(file_bytes, cv2.IMREAD_COLOR)\n",
    "\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "\n",
    "        if len(faces) > 0:\n",
    "            (x,y,w,h) = faces[0]\n",
    "            face = img[y:y+h, x:x+w]\n",
    "\n",
    "            results = predict_attributes(face)\n",
    "\n",
    "            cv2.rectangle(img, (x,y), (x+w,y+h), (0,255,0), 2)\n",
    "            st.image(img, caption=\"Detected Face\", channels=\"BGR\")\n",
    "\n",
    "            st.subheader(\"🔮 Predictions\")\n",
    "            for key, (label, conf) in results.items():\n",
    "                st.markdown(\n",
    "                    f'<div class=\"result-card\"><b>{key}</b>: {label}</div>',\n",
    "                    unsafe_allow_html=True\n",
    "                )\n",
    "                st.progress(conf)\n",
    "\n",
    "        else:\n",
    "            st.warning(\" No face detected!\")\n",
    "\n",
    "\n",
    "with tab2:\n",
    "    st.write(\"Enable webcam to see live predictions in real-time\")\n",
    "    run = st.checkbox(\"Start Webcam\")\n",
    "    FRAME_WINDOW = st.image([])\n",
    "    results_placeholder = st.empty()\n",
    "\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    cap.set(3, 320)\n",
    "    cap.set(4, 240)\n",
    "\n",
    "    frame_skip = 3\n",
    "    count = 0\n",
    "\n",
    "    while run:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            st.error(\"Failed to capture image\")\n",
    "            break\n",
    "\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        count += 1\n",
    "\n",
    "        if count % frame_skip == 0:\n",
    "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "\n",
    "            if len(faces) > 0:\n",
    "                (x,y,w,h) = faces[0]\n",
    "                face = frame[y:y+h, x:x+w]\n",
    "                results = predict_attributes(face)\n",
    "\n",
    "                cv2.rectangle(frame, (x,y), (x+w,y+h), (0,255,0), 2)\n",
    "\n",
    "                with results_placeholder.container():\n",
    "                    st.subheader(\"🔮 Live Predictions\")\n",
    "                    for key, (label, conf) in results.items():\n",
    "                        st.markdown(\n",
    "                            f'<div class=\"result-card\"><b>{key}</b>: {label}</div>',\n",
    "                            unsafe_allow_html=True\n",
    "                        )\n",
    "                        st.progress(conf)\n",
    "\n",
    "        FRAME_WINDOW.image(frame, channels=\"BGR\")\n",
    "\n",
    "    cap.release()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
